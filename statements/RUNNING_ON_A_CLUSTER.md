# Running on a cluster

At this point, you are well poised to tackle a production Spark use case or at least execute your already implemented exercises on a cluster. We've covered Spark's configuration management, transformations, actions, read and write formats, pseudo set operations, etc.
 
Today we are going to execute some code on a cluster. Could you implement the following tasks using Apache Spark?

* Using Docker, initialize a Spark instance locally and submit your exercises.
* Using Databricks community edition, create a cluster and execute your exercises.
 
 **Remember to take a look at the Spark UI and review if we could improve any execution.** 