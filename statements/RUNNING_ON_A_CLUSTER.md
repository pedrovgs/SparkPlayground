# Running on a cluster

At this point, you are well poised to tackle a production Spark use case or at least execute your already implemented exercises on a cluster. We've covered Spark's configuration management, transformations, actions, read and write formats, pseudo set operations, etc.
 
Today we are going to execute some code on a cluster. Could you implement the following tasks using Apache Spark?

* Using your local Spark installation, execute your playground using ``spark-submit``. 
* Using Docker, initialize a Spark instance locally and submit your exercises.
 
**Remember to take a look at the Spark UI and review if we could improve any execution.** 